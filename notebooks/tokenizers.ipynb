{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wZI1LzbAUMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca06a777-5142-4b2b-d956-93d6f45aaf8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer\n",
        "from collections import Counter\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set path to your data directory\n",
        "base_dir = '/content/drive/MyDrive/preprocessed/data'  # Update if needed\n",
        "print(f\"Looking for data in: {base_dir}\")\n",
        "\n",
        "# Function to read CSV files with Twitter data\n",
        "def load_twitter_data(file_path):\n",
        "    \"\"\"Load Twitter data from CSV files.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Successfully loaded {file_path}\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        print(\"\\nSample data:\")\n",
        "        print(df.head(3))\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process column names and structure\n",
        "def process_twitter_dataframe(df):\n",
        "    \"\"\"Process and standardize Twitter dataframe structure.\"\"\"\n",
        "\n",
        "    # Check if the DataFrame is valid\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"Empty or invalid dataframe\")\n",
        "        return None\n",
        "\n",
        "    # Case 1: If df has proper column names\n",
        "    if 'text' in df.columns and 'user_id' in df.columns:\n",
        "        print(\"DataFrame already has proper column names\")\n",
        "        return df\n",
        "\n",
        "    # Case 2: If it has two columns but unnamed or numbered\n",
        "    if df.shape[1] == 2:\n",
        "        print(\"Renaming columns to 'text' and 'user_id'\")\n",
        "        df.columns = ['text', 'user_id']\n",
        "        return df\n",
        "\n",
        "    # This handles the case where columns are numeric (0,1) or unnamed\n",
        "    print(\"Attempting to extract text and user_id from data...\")\n",
        "\n",
        "    # Identify which column contains the text\n",
        "    text_col = None\n",
        "    user_id_col = None\n",
        "\n",
        "    # Look at first row to determine which column has text vs ID\n",
        "    for col in df.columns:\n",
        "        first_val = str(df[col].iloc[0])\n",
        "        if first_val.startswith('b\\'@') or first_val.startswith('b\"@') or 'http' in first_val:\n",
        "            text_col = col\n",
        "        elif first_val.isdigit() or (first_val.isalnum() and len(first_val) > 7):\n",
        "            user_id_col = col\n",
        "\n",
        "    # If we found both columns, create a new DataFrame with proper column names\n",
        "    if text_col is not None and user_id_col is not None:\n",
        "        new_df = pd.DataFrame({\n",
        "            'text': df[text_col],\n",
        "            'user_id': df[user_id_col]\n",
        "        })\n",
        "        print(f\"Created new DataFrame with 'text' and 'user_id' columns\")\n",
        "        return new_df\n",
        "\n",
        "    # If columns not clearly identified, try first two columns\n",
        "    if df.shape[1] >= 2:\n",
        "        print(\"Using first two columns as text and user_id\")\n",
        "        new_df = pd.DataFrame({\n",
        "            'text': df.iloc[:, 0],\n",
        "            'user_id': df.iloc[:, 1]\n",
        "        })\n",
        "        return new_df\n",
        "\n",
        "    print(\"Could not determine the proper structure of the dataframe\")\n",
        "    return None\n",
        "\n",
        "# Clean and preprocess Twitter text\n",
        "def clean_twitter_text(text):\n",
        "    \"\"\"Clean and normalize Twitter text for authorship attribution.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Handle byte string format (b'text')\n",
        "    if isinstance(text, str) and text.startswith(\"b'\") or text.startswith('b\"'):\n",
        "        # Extract content from byte string representation\n",
        "        text = text[2:-1]  # Remove b' and closing '\n",
        "\n",
        "    # Convert to string if it's not already\n",
        "    text = str(text)\n",
        "    # Replace escaped newlines with space\n",
        "    text = text.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
        "    # Replace user mentions (keep as feature for authorship)\n",
        "    text = re.sub(r'@\\w+', '[USER]', text)\n",
        "    # Replace hashtags (keep as feature for authorship)\n",
        "    text = re.sub(r'#(\\w+)', r'[HASHTAG] \\1', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Load the data files\n",
        "data_files = [\n",
        "    os.path.join(base_dir, '100_users_with_200_tweets.csv'),\n",
        "    os.path.join(base_dir, '200_users_with_200_tweets.csv'),\n",
        "    os.path.join(base_dir, '500_users_with_200_tweets.csv')\n",
        "]\n",
        "\n",
        "# Load and combine data from all files\n",
        "all_data = []\n",
        "for file_path in data_files:\n",
        "    if os.path.exists(file_path):\n",
        "        df = load_twitter_data(file_path)\n",
        "        if df is not None:\n",
        "            processed_df = process_twitter_dataframe(df)\n",
        "            if processed_df is not None:\n",
        "                all_data.append(processed_df)\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "# Combine all dataframes\n",
        "if all_data:\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    print(f\"\\nCombined data: {combined_df.shape[0]} tweets from multiple files\")\n",
        "else:\n",
        "    print(\"No valid data loaded. Please check the file paths and formats.\")\n",
        "    exit()\n",
        "\n",
        "# Clean the text data\n",
        "print(\"\\nCleaning text data...\")\n",
        "combined_df['cleaned_text'] = combined_df['text'].apply(clean_twitter_text)\n",
        "\n",
        "# Basic data analysis\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(f\"Total tweets: {len(combined_df)}\")\n",
        "print(f\"Unique authors: {combined_df['user_id'].nunique()}\")\n",
        "\n",
        "# Calculate tweets per author\n",
        "tweets_per_author = combined_df['user_id'].value_counts()\n",
        "print(f\"\\nTweets per author statistics:\")\n",
        "print(f\"Min: {tweets_per_author.min()}\")\n",
        "print(f\"Max: {tweets_per_author.max()}\")\n",
        "print(f\"Average: {tweets_per_author.mean():.2f}\")\n",
        "\n",
        "# Visualize tweets per author distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(tweets_per_author, kde=True)\n",
        "plt.title('Distribution of Tweets per Author')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Number of Authors')\n",
        "plt.savefig('tweets_per_author.png')\n",
        "plt.close()\n",
        "\n",
        "# Filter authors with a minimum number of tweets for reliable attribution\n",
        "min_tweets = 50  # Authors should have at least 50 tweets for reliable attribution\n",
        "authors_to_keep = tweets_per_author[tweets_per_author >= min_tweets].index.tolist()\n",
        "filtered_df = combined_df[combined_df['user_id'].isin(authors_to_keep)].copy()\n",
        "\n",
        "print(f\"\\nFiltered to {len(filtered_df)} tweets from {len(authors_to_keep)} authors with {min_tweets}+ tweets each\")\n",
        "\n",
        "# Determine which authors to use for the final dataset\n",
        "# For computational feasibility, we'll limit to a maximum number of authors\n",
        "max_authors = 100  # Adjust based on computational resources\n",
        "if len(authors_to_keep) > max_authors:\n",
        "    # Sort authors by number of tweets (descending) and take the top max_authors\n",
        "    top_authors = tweets_per_author.nlargest(max_authors).index.tolist()\n",
        "    filtered_df = combined_df[combined_df['user_id'].isin(top_authors)].copy()\n",
        "    print(f\"Limited to top {max_authors} authors with most tweets\")\n",
        "\n",
        "# Ensure balanced dataset by sampling the same number of tweets per author\n",
        "tweets_per_author_after_filtering = filtered_df['user_id'].value_counts()\n",
        "min_tweets_per_author = tweets_per_author_after_filtering.min()\n",
        "print(f\"Minimum tweets per author after filtering: {min_tweets_per_author}\")\n",
        "\n",
        "# Sample equal number of tweets per author\n",
        "balanced_df = pd.DataFrame()\n",
        "for author in filtered_df['user_id'].unique():\n",
        "    author_tweets = filtered_df[filtered_df['user_id'] == author]\n",
        "    # Sample min_tweets_per_author or all tweets if less\n",
        "    if len(author_tweets) > min_tweets_per_author:\n",
        "        sampled_tweets = author_tweets.sample(min_tweets_per_author, random_state=42)\n",
        "    else:\n",
        "        sampled_tweets = author_tweets\n",
        "    balanced_df = pd.concat([balanced_df, sampled_tweets], ignore_index=True)\n",
        "\n",
        "print(f\"Balanced dataset: {len(balanced_df)} tweets from {balanced_df['user_id'].nunique()} authors\")\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "# Stratify by author to ensure all authors are represented in each split\n",
        "train_df, temp_df = train_test_split(\n",
        "    balanced_df,\n",
        "    test_size=0.3,\n",
        "    stratify=balanced_df['user_id'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_df['user_id'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {len(train_df)} tweets\")\n",
        "print(f\"Validation set: {len(val_df)} tweets\")\n",
        "print(f\"Test set: {len(test_df)} tweets\")\n",
        "\n",
        "# Verify author distribution across splits\n",
        "print(\"\\nVerifying author distribution across splits...\")\n",
        "train_authors = train_df['user_id'].value_counts()\n",
        "val_authors = val_df['user_id'].value_counts()\n",
        "test_authors = test_df['user_id'].value_counts()\n",
        "\n",
        "print(f\"Authors in train set: {len(train_authors)}\")\n",
        "print(f\"Authors in validation set: {len(val_authors)}\")\n",
        "print(f\"Authors in test set: {len(test_authors)}\")\n",
        "\n",
        "# Create author ID mapping (consistent across all splits)\n",
        "all_authors = sorted(balanced_df['user_id'].unique())\n",
        "author_to_id = {author: idx for idx, author in enumerate(all_authors)}\n",
        "\n",
        "# Add numeric author IDs to dataframes\n",
        "train_df['author_id'] = train_df['user_id'].map(author_to_id)\n",
        "val_df['author_id'] = val_df['user_id'].map(author_to_id)\n",
        "test_df['author_id'] = test_df['user_id'].map(author_to_id)\n",
        "\n",
        "# Load tokenizers for different subword methods\n",
        "print(\"\\nLoading tokenizers...\")\n",
        "# 1. BPE (Byte-Pair Encoding)\n",
        "xlm_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "# 2. WordPiece\n",
        "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# 3. SentencePiece\n",
        "sentencepiece_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "\n",
        "# Function to tokenize using different methods\n",
        "def tokenize_text(text, tokenizer):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Sample data for tokenization comparison\n",
        "print(\"\\nTokenizing sample data with different methods...\")\n",
        "sample_size = min(100, len(train_df))\n",
        "sample_data = train_df.sample(sample_size)\n",
        "\n",
        "# Apply tokenization methods\n",
        "sample_data['bpe_tokens'] = sample_data['cleaned_text'].apply(lambda x: tokenize_text(x, xlm_tokenizer))\n",
        "sample_data['wordpiece_tokens'] = sample_data['cleaned_text'].apply(lambda x: tokenize_text(x, mbert_tokenizer))\n",
        "sample_data['sentencepiece_tokens'] = sample_data['cleaned_text'].apply(lambda x: tokenize_text(x, sentencepiece_tokenizer))\n",
        "\n",
        "# Calculate token counts\n",
        "sample_data['bpe_token_count'] = sample_data['bpe_tokens'].apply(len)\n",
        "sample_data['wordpiece_token_count'] = sample_data['wordpiece_tokens'].apply(len)\n",
        "sample_data['sentencepiece_token_count'] = sample_data['sentencepiece_tokens'].apply(len)\n",
        "\n",
        "# Print token count statistics\n",
        "print(\"\\nToken count statistics:\")\n",
        "for method in ['bpe', 'wordpiece', 'sentencepiece']:\n",
        "    counts = sample_data[f'{method}_token_count']\n",
        "    print(f\"{method.capitalize()} tokens: min={counts.min()}, max={counts.max()}, avg={counts.mean():.2f}\")\n",
        "\n",
        "# Visualize token count distributions\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(sample_data['bpe_token_count'], kde=True)\n",
        "plt.title('BPE Token Count Distribution')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(sample_data['wordpiece_token_count'], kde=True)\n",
        "plt.title('WordPiece Token Count Distribution')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(sample_data['sentencepiece_token_count'], kde=True)\n",
        "plt.title('SentencePiece Token Count Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('token_distributions.png')\n",
        "plt.close()\n",
        "\n",
        "# Character n-gram analysis (important for authorship attribution)\n",
        "def extract_ngrams(text, n=2):\n",
        "    \"\"\"Extract character n-grams from text\"\"\"\n",
        "    text = str(text).lower()\n",
        "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
        "\n",
        "# Add n-gram extraction to a sample of the data\n",
        "print(\"\\nPerforming n-gram analysis for authorship features...\")\n",
        "ngram_sample = train_df.sample(min(500, len(train_df)))\n",
        "ngram_sample['char_2grams'] = ngram_sample['cleaned_text'].apply(lambda x: extract_ngrams(x, 2))\n",
        "ngram_sample['char_3grams'] = ngram_sample['cleaned_text'].apply(lambda x: extract_ngrams(x, 3))\n",
        "ngram_sample['char_4grams'] = ngram_sample['cleaned_text'].apply(lambda x: extract_ngrams(x, 4))\n",
        "\n",
        "# Count the most common n-grams\n",
        "all_2grams = [gram for sublist in ngram_sample['char_2grams'] for gram in sublist]\n",
        "all_3grams = [gram for sublist in ngram_sample['char_3grams'] for gram in sublist]\n",
        "all_4grams = [gram for sublist in ngram_sample['char_4grams'] for gram in sublist]\n",
        "\n",
        "# Get the 20 most common n-grams\n",
        "common_2grams = Counter(all_2grams).most_common(20)\n",
        "common_3grams = Counter(all_3grams).most_common(20)\n",
        "common_4grams = Counter(all_4grams).most_common(20)\n",
        "\n",
        "# Plot the most common n-grams\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.barplot(x=[x[0] for x in common_2grams], y=[x[1] for x in common_2grams])\n",
        "plt.title('Most Common Character 2-grams')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.barplot(x=[x[0] for x in common_3grams], y=[x[1] for x in common_3grams])\n",
        "plt.title('Most Common Character 3-grams')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.barplot(x=[x[0] for x in common_4grams], y=[x[1] for x in common_4grams])\n",
        "plt.title('Most Common Character 4-grams')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('common_ngrams.png')\n",
        "plt.close()\n",
        "\n",
        "# Analyze authorship patterns - find distinctive n-grams for authors\n",
        "print(\"\\nAnalyzing distinctive n-grams by author...\")\n",
        "\n",
        "# Function to calculate author distinctiveness based on n-grams\n",
        "def get_author_distinctive_ngrams(df, n=3, top_k=10):\n",
        "    \"\"\"Find the most distinctive n-grams for each author\"\"\"\n",
        "    # Add n-gram column if it doesn't exist\n",
        "    ngram_col = f'char_{n}grams'\n",
        "    if ngram_col not in df.columns:\n",
        "        df[ngram_col] = df['cleaned_text'].apply(lambda x: extract_ngrams(x, n))\n",
        "\n",
        "    # Get global n-gram frequencies\n",
        "    all_ngrams = [gram for sublist in df[ngram_col] for gram in sublist]\n",
        "    global_freq = Counter(all_ngrams)\n",
        "    total_ngrams = sum(global_freq.values())\n",
        "    global_prob = {gram: count/total_ngrams for gram, count in global_freq.items()}\n",
        "\n",
        "    # Find distinctive n-grams for each author\n",
        "    author_distinctive_ngrams = {}\n",
        "\n",
        "    for author in df['user_id'].unique():\n",
        "        author_texts = df[df['user_id'] == author]\n",
        "        author_ngrams = [gram for sublist in author_texts[ngram_col] for gram in sublist]\n",
        "        author_freq = Counter(author_ngrams)\n",
        "        author_total = sum(author_freq.values())\n",
        "\n",
        "        # Calculate distinctiveness score (ratio of author frequency to global frequency)\n",
        "        ngram_scores = {}\n",
        "        for gram, count in author_freq.items():\n",
        "            author_prob = count / author_total\n",
        "            global_prob_gram = global_prob.get(gram, 1e-10)  # Avoid division by zero\n",
        "            distinctiveness = author_prob / global_prob_gram\n",
        "            ngram_scores[gram] = distinctiveness\n",
        "\n",
        "        # Get top k distinctive n-grams\n",
        "        top_ngrams = sorted(ngram_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        author_distinctive_ngrams[author] = top_ngrams\n",
        "\n",
        "    return author_distinctive_ngrams\n",
        "\n",
        "# Get distinctive 3-grams for a sample of authors\n",
        "sample_authors = list(train_df['user_id'].unique())[:5]  # Just first 5 authors for demonstration\n",
        "sample_author_df = train_df[train_df['user_id'].isin(sample_authors)]\n",
        "distinctive_3grams = get_author_distinctive_ngrams(sample_author_df, n=3, top_k=10)\n",
        "\n",
        "# Visualize distinctive n-grams for sample authors\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, (author, ngrams) in enumerate(distinctive_3grams.items()):\n",
        "    if i >= 4:  # Only show first 4 authors to avoid crowding\n",
        "        break\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.bar([x[0] for x in ngrams[:8]], [x[1] for x in ngrams[:8]])\n",
        "    plt.title(f'Author {author}: Distinctive 3-grams')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "plt.savefig('author_distinctive_ngrams.png')\n",
        "plt.close()\n",
        "\n",
        "# Function to prepare data for transformer models\n",
        "def prepare_data_for_transformers(df, tokenizer, max_length=128):\n",
        "    \"\"\"Convert dataframe to tokenized inputs for transformer models\"\"\"\n",
        "    # Ensure all authors have numeric IDs\n",
        "    if 'author_id' not in df.columns:\n",
        "        all_authors = sorted(df['user_id'].unique())\n",
        "        author_to_id = {author: idx for idx, author in enumerate(all_authors)}\n",
        "        df['author_id'] = df['user_id'].map(author_to_id)\n",
        "\n",
        "    # Get author mapping (even if already exists, for reference)\n",
        "    author_map = {author: id for author, id in zip(df['user_id'], df['author_id'])}\n",
        "\n",
        "    # Tokenize texts\n",
        "    encodings = tokenizer(\n",
        "        df['cleaned_text'].tolist(),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create labels tensor\n",
        "    labels = torch.tensor(df['author_id'].values)\n",
        "\n",
        "    return {\n",
        "        'input_ids': encodings.input_ids,\n",
        "        'attention_mask': encodings.attention_mask,\n",
        "        'labels': labels,\n",
        "        'author_map': author_map\n",
        "    }\n",
        "\n",
        "# Create datasets for each tokenization method\n",
        "print(\"\\nPreparing datasets for transformer models...\")\n",
        "\n",
        "print(\"Processing with BPE tokenization (XLM-RoBERTa)...\")\n",
        "bpe_train = prepare_data_for_transformers(train_df, xlm_tokenizer)\n",
        "bpe_val = prepare_data_for_transformers(val_df, xlm_tokenizer)\n",
        "bpe_test = prepare_data_for_transformers(test_df, xlm_tokenizer)\n",
        "\n",
        "print(\"Processing with WordPiece tokenization (mBERT)...\")\n",
        "wordpiece_train = prepare_data_for_transformers(train_df, mbert_tokenizer)\n",
        "wordpiece_val = prepare_data_for_transformers(val_df, mbert_tokenizer)\n",
        "wordpiece_test = prepare_data_for_transformers(test_df, mbert_tokenizer)\n",
        "\n",
        "print(\"Processing with SentencePiece tokenization (XLM-RoBERTa-Large)...\")\n",
        "sentencepiece_train = prepare_data_for_transformers(train_df, sentencepiece_tokenizer)\n",
        "sentencepiece_val = prepare_data_for_transformers(val_df, sentencepiece_tokenizer)\n",
        "sentencepiece_test = prepare_data_for_transformers(test_df, sentencepiece_tokenizer)\n",
        "\n",
        "# Save the preprocessed datasets\n",
        "print(\"\\nSaving preprocessed datasets...\")\n",
        "\n",
        "# Create directory for output if it doesn't exist\n",
        "output_dir = \"/content/drive/MyDrive/preprocessed_output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save BPE datasets\n",
        "torch.save(bpe_train, os.path.join(output_dir, 'bpe_train_dataset.pt'))\n",
        "torch.save(bpe_val, os.path.join(output_dir, 'bpe_val_dataset.pt'))\n",
        "torch.save(bpe_test, os.path.join(output_dir, 'bpe_test_dataset.pt'))\n",
        "\n",
        "# Save WordPiece datasets\n",
        "torch.save(wordpiece_train, os.path.join(output_dir, 'wordpiece_train_dataset.pt'))\n",
        "torch.save(wordpiece_val, os.path.join(output_dir, 'wordpiece_val_dataset.pt'))\n",
        "torch.save(wordpiece_test, os.path.join(output_dir, 'wordpiece_test_dataset.pt'))\n",
        "\n",
        "# Save SentencePiece datasets\n",
        "torch.save(sentencepiece_train, os.path.join(output_dir, 'sentencepiece_train_dataset.pt'))\n",
        "torch.save(sentencepiece_val, os.path.join(output_dir, 'sentencepiece_val_dataset.pt'))\n",
        "torch.save(sentencepiece_test, os.path.join(output_dir, 'sentencepiece_test_dataset.pt'))\n",
        "\n",
        "# Save original processed DataFrames for reference\n",
        "train_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
        "val_df.to_csv(os.path.join(output_dir, 'val.csv'), index=False)\n",
        "test_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n",
        "\n",
        "# Calculate author statistics for the README\n",
        "author_tweet_counts = balanced_df['user_id'].value_counts()\n",
        "min_count = author_tweet_counts.min()\n",
        "max_count = author_tweet_counts.max()\n",
        "avg_count = author_tweet_counts.mean()\n",
        "num_authors = len(author_tweet_counts)\n",
        "\n",
        "\n",
        "print(f\"\\nPreprocessing complete! All datasets saved to {output_dir}\")\n",
        "print(\"\\nDatasets have been preprocessed using three different subword tokenization methods:\")\n",
        "print(\"1. Byte-Pair Encoding (BPE) - XLM-RoBERTa Base\")\n",
        "print(\"2. WordPiece - mBERT\")\n",
        "print(\"3. SentencePiece - XLM-RoBERTa Large\")\n",
        "print(\"\\nEach dataset includes:\")\n",
        "print(\"- Tokenized text data\")\n",
        "print(\"- Real author ID mappings\")\n",
        "print(\"- Ready-to-use format for transformer models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6whcckXqNNT2",
        "outputId": "0accf14b-e5a1-4d05-b3e2-867e28990d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for data in: /content/drive/MyDrive/preprocessed/data\n",
            "Successfully loaded /content/drive/MyDrive/preprocessed/data/100_users_with_200_tweets.csv\n",
            "Shape: (20000, 2)\n",
            "Columns: ['0', '1']\n",
            "\n",
            "Sample data:\n",
            "                                                   0         1\n",
            "0  b'@justinluv4evz33 yeah a little bit but not v...  88342713\n",
            "1  b\"@loveeejustin haha well people said that I'm...  88342713\n",
            "2                    b'One more test next hour...\\n'  88342713\n",
            "Renaming columns to 'text' and 'user_id'\n",
            "Successfully loaded /content/drive/MyDrive/preprocessed/data/200_users_with_200_tweets.csv\n",
            "Shape: (40000, 2)\n",
            "Columns: ['0', '1']\n",
            "\n",
            "Sample data:\n",
            "                                                   0         1\n",
            "0                       b'@RogueLab checking Yes!\\n'  59518710\n",
            "1  b'@marleyterrier Hi Marley! how are you today?\\n'  59518710\n",
            "2         b'@CokieTheCat I have a kitteh brother?\\n'  59518710\n",
            "Renaming columns to 'text' and 'user_id'\n",
            "Successfully loaded /content/drive/MyDrive/preprocessed/data/500_users_with_200_tweets.csv\n",
            "Shape: (100000, 2)\n",
            "Columns: ['0', '1']\n",
            "\n",
            "Sample data:\n",
            "                                                   0         1\n",
            "0  b'@likewhoaitsnixx omg! Big bang in your ds to...  29934775\n",
            "1  b'@JanneJanne thats fine... Majority of profil...  29934775\n",
            "2  b'Nothing to do online.... Might go to bed ear...  29934775\n",
            "Renaming columns to 'text' and 'user_id'\n",
            "\n",
            "Combined data: 160000 tweets from multiple files\n",
            "\n",
            "Cleaning text data...\n",
            "\n",
            "Basic statistics:\n",
            "Total tweets: 160000\n",
            "Unique authors: 769\n",
            "\n",
            "Tweets per author statistics:\n",
            "Min: 200\n",
            "Max: 600\n",
            "Average: 208.06\n",
            "\n",
            "Filtered to 160000 tweets from 769 authors with 50+ tweets each\n",
            "Limited to top 100 authors with most tweets\n",
            "Minimum tweets per author after filtering: 200\n",
            "Balanced dataset: 20000 tweets from 100 authors\n",
            "\n",
            "Train set: 14000 tweets\n",
            "Validation set: 3000 tweets\n",
            "Test set: 3000 tweets\n",
            "\n",
            "Verifying author distribution across splits...\n",
            "Authors in train set: 100\n",
            "Authors in validation set: 100\n",
            "Authors in test set: 100\n",
            "\n",
            "Loading tokenizers...\n",
            "\n",
            "Tokenizing sample data with different methods...\n",
            "\n",
            "Token count statistics:\n",
            "Bpe tokens: min=7, max=44, avg=23.44\n",
            "Wordpiece tokens: min=8, max=45, avg=24.68\n",
            "Sentencepiece tokens: min=7, max=44, avg=23.44\n",
            "\n",
            "Performing n-gram analysis for authorship features...\n",
            "\n",
            "Analyzing distinctive n-grams by author...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4138407807>:335: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[ngram_col] = df['cleaned_text'].apply(lambda x: extract_ngrams(x, n))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preparing datasets for transformer models...\n",
            "Processing with BPE tokenization (XLM-RoBERTa)...\n",
            "Processing with WordPiece tokenization (mBERT)...\n",
            "Processing with SentencePiece tokenization (XLM-RoBERTa-Large)...\n",
            "\n",
            "Saving preprocessed datasets...\n",
            "\n",
            "Preprocessing complete! All datasets saved to /content/drive/MyDrive/preprocessed_output\n",
            "\n",
            "Datasets have been preprocessed using three different subword tokenization methods:\n",
            "1. Byte-Pair Encoding (BPE) - XLM-RoBERTa Base\n",
            "2. WordPiece - mBERT\n",
            "3. SentencePiece - XLM-RoBERTa Large\n",
            "\n",
            "Each dataset includes:\n",
            "- Tokenized text data\n",
            "- Real author ID mappings\n",
            "- Ready-to-use format for transformer models\n"
          ]
        }
      ]
    }
  ]
}